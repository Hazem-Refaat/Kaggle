{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/hazemegy/bone-fracture-classification-using-vit?scriptVersionId=179103589\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"5d135a87","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-22T10:17:14.661353Z","iopub.status.busy":"2024-05-22T10:17:14.661078Z","iopub.status.idle":"2024-05-22T10:17:15.379671Z","shell.execute_reply":"2024-05-22T10:17:15.378739Z"},"papermill":{"duration":0.725407,"end_time":"2024-05-22T10:17:15.382031","exception":false,"start_time":"2024-05-22T10:17:14.656624","status":"completed"},"tags":[]},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","import os\n","import glob"]},{"cell_type":"code","execution_count":2,"id":"9546a663","metadata":{"execution":{"iopub.execute_input":"2024-05-22T10:17:15.389842Z","iopub.status.busy":"2024-05-22T10:17:15.389274Z","iopub.status.idle":"2024-05-22T10:19:08.838056Z","shell.execute_reply":"2024-05-22T10:19:08.836928Z"},"papermill":{"duration":113.455343,"end_time":"2024-05-22T10:19:08.840689","exception":false,"start_time":"2024-05-22T10:17:15.385346","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-22 10:17:17.156778: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-22 10:17:17.156872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-22 10:17:17.292727: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004347.jpg due to image file is truncated (40 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004148.jpg due to image file is truncated (14 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004134.jpg due to image file is truncated (1 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004149.jpg due to image file is truncated (33 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004143.jpg due to image file is truncated (10 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train/not fractured/IMG0004308.jpg due to image file is truncated (40 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004347.jpg due to image file is truncated (40 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004148.jpg due to image file is truncated (14 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004134.jpg due to image file is truncated (1 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004149.jpg due to image file is truncated (33 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004143.jpg due to image file is truncated (10 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val/not fractured/IMG0004308.jpg due to image file is truncated (40 bytes not processed)\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004347.jpg due to image file is truncated (40 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004148.jpg due to image file is truncated (14 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004134.jpg due to image file is truncated (1 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004149.jpg due to image file is truncated (33 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004143.jpg due to image file is truncated (10 bytes not processed)\n","Skipping corrupted image: /kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test/not fractured/IMG0004308.jpg due to image file is truncated (40 bytes not processed)\n"]}],"source":["import os\n","import numpy as np\n","from PIL import Image\n","from tensorflow.keras.utils import Sequence\n","\n","class CustomImageDataGenerator(Sequence):\n","    def __init__(self, image_paths, labels, batch_size, target_size, rescale):\n","        self.image_paths = image_paths\n","        self.labels = labels\n","        self.batch_size = batch_size\n","        self.target_size = target_size\n","        self.rescale = rescale\n","        self.indices = np.arange(len(self.image_paths))\n","\n","    def __len__(self):\n","        return int(np.ceil(len(self.image_paths) / self.batch_size))\n","\n","    def __getitem__(self, idx):\n","        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        batch_x = [self.load_image(self.image_paths[i]) for i in batch_indices]\n","        batch_y = [self.labels[i] for i in batch_indices]\n","\n","        return np.array(batch_x), np.array(batch_y)\n","\n","    def load_image(self, image_path):\n","        try:\n","            img = Image.open(image_path)\n","            if img.mode != 'RGB':\n","                img = img.convert('RGB')\n","            img = img.resize(self.target_size)\n","            img_array = np.array(img)\n","            img_array = img_array * self.rescale\n","            return img_array\n","        except (OSError, ValueError) as e:\n","            print(f\"Error loading image {image_path}: {e}\")\n","            return np.zeros((self.target_size[0], self.target_size[1], 3))  # Return a black image of the target size\n","\n","    def on_epoch_end(self):\n","        np.random.shuffle(self.indices)\n","\n","def filter_truncated_images(directory, target_size):\n","    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n","    image_paths = []\n","    for root, _, files in os.walk(directory):\n","        for file in files:\n","            if any(file.lower().endswith(ext) for ext in image_extensions):\n","                image_path = os.path.join(root, file)\n","                try:\n","                    img = Image.open(image_path)\n","                    if img.mode != 'RGB':\n","                        img = img.convert('RGB')\n","                    img = img.resize(target_size)\n","                    img_array = np.array(img)\n","                    img_array = img_array * 1./255  # Rescale the image\n","                    image_paths.append(image_path)\n","                except (OSError, ValueError) as e:\n","                    print(f\"Skipping corrupted image: {image_path} due to {e}\")\n","    return image_paths\n","\n","# Example usage\n","train_image_paths = filter_truncated_images('/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train', target_size=(224, 224))\n","val_image_paths = filter_truncated_images('/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val', target_size=(224, 224))\n","test_image_paths = filter_truncated_images('/kaggle/input/fracture-multi-region-x-ray-data/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test', target_size=(224, 224))\n","\n","# Generate labels based on file paths\n","train_labels = [0 if 'not fractured' in path else 1 for path in train_image_paths]\n","val_labels = [0 if 'not fractured' in path else 1 for path in val_image_paths]\n","test_labels = [0 if 'not fractured' in path else 1 for path in test_image_paths]\n","\n","batch_size = 32  \n","target_size = (224, 224) \n","\n","train_generator = CustomImageDataGenerator(train_image_paths, train_labels, batch_size, target_size, rescale=1./255)\n","val_generator = CustomImageDataGenerator(val_image_paths, val_labels, batch_size, target_size, rescale=1./255)\n","test_generator = CustomImageDataGenerator(test_image_paths, test_labels, batch_size, target_size, rescale=1./255)\n"]},{"cell_type":"code","execution_count":3,"id":"d5756d91","metadata":{"execution":{"iopub.execute_input":"2024-05-22T10:19:08.851909Z","iopub.status.busy":"2024-05-22T10:19:08.851315Z","iopub.status.idle":"2024-05-22T10:19:09.066619Z","shell.execute_reply":"2024-05-22T10:19:09.065643Z"},"papermill":{"duration":0.223178,"end_time":"2024-05-22T10:19:09.06872","exception":false,"start_time":"2024-05-22T10:19:08.845542","status":"completed"},"tags":[]},"outputs":[],"source":["import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)"]},{"cell_type":"code","execution_count":4,"id":"c18f406c","metadata":{"execution":{"iopub.execute_input":"2024-05-22T10:19:09.079913Z","iopub.status.busy":"2024-05-22T10:19:09.079574Z","iopub.status.idle":"2024-05-22T10:19:09.102151Z","shell.execute_reply":"2024-05-22T10:19:09.10125Z"},"papermill":{"duration":0.030692,"end_time":"2024-05-22T10:19:09.10431","exception":false,"start_time":"2024-05-22T10:19:09.073618","status":"completed"},"tags":[]},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","class PatchEmbedding(layers.Layer):\n","    def __init__(self, patch_size, embed_dim):\n","        super(PatchEmbedding, self).__init__()\n","        self.patch_size = patch_size\n","        self.embed_dim = embed_dim\n","        self.proj = layers.Dense(embed_dim)\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding='VALID'\n","        )\n","        patch_dims = self.patch_size * self.patch_size * tf.shape(images)[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return self.proj(patches)\n","\n","class MultiHeadSelfAttention(layers.Layer):\n","    def __init__(self, num_heads, embed_dim):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","\n","    def call(self, inputs):\n","        return self.attention(inputs, inputs)\n","\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate):\n","        super(TransformerBlock, self).__init__()\n","        self.attention = MultiHeadSelfAttention(num_heads, embed_dim)\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.mlp = tf.keras.Sequential([\n","            layers.Dense(mlp_dim, activation='gelu'),\n","            layers.Dropout(dropout_rate),\n","            layers.Dense(embed_dim),\n","            layers.Dropout(dropout_rate)\n","        ])\n","\n","    def call(self, inputs):\n","        x = self.layernorm1(inputs)\n","        x = self.attention(x)\n","        x = x + inputs\n","        x = self.layernorm2(x)\n","        x = self.mlp(x)\n","        return x + inputs\n","\n","class VisionTransformer(tf.keras.Model):\n","    def __init__(self, image_size, patch_size, embed_dim, num_heads, num_blocks, mlp_dim, num_classes, dropout_rate=0.1):\n","        super(VisionTransformer, self).__init__()\n","        self.patch_embed = PatchEmbedding(patch_size, embed_dim)\n","        height, width, _ = image_size\n","        num_patches = (height // patch_size) * (width // patch_size)\n","        self.pos_embed = self.add_weight(name=\"pos_embed\", shape=(1, num_patches + 1, embed_dim), initializer=tf.initializers.RandomNormal(stddev=0.02), trainable=True)\n","        self.cls_token = self.add_weight(name=\"cls_token\", shape=(1, 1, embed_dim), initializer=tf.initializers.RandomNormal(stddev=0.02), trainable=True)\n","        self.dropout = layers.Dropout(dropout_rate)\n","        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, mlp_dim, dropout_rate) for _ in range(num_blocks)]\n","        self.layernorm = layers.LayerNormalization(epsilon=1e-6)\n","        self.classifier = layers.Dense(num_classes)\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = self.patch_embed(images)\n","        cls_tokens = tf.broadcast_to(self.cls_token, [batch_size, 1, tf.shape(patches)[-1]])\n","        x = tf.concat([cls_tokens, patches], axis=1)\n","        x = x + self.pos_embed\n","        x = self.dropout(x)\n","        for block in self.transformer_blocks:\n","            x = block(x)\n","        x = self.layernorm(x)\n","        cls_token_final = x[:, 0]  # Extract the cls_token for classification\n","        return self.classifier(cls_token_final)"]},{"cell_type":"code","execution_count":5,"id":"b8b4a9f6","metadata":{"execution":{"iopub.execute_input":"2024-05-22T10:19:09.114738Z","iopub.status.busy":"2024-05-22T10:19:09.114459Z","iopub.status.idle":"2024-05-22T10:19:09.393884Z","shell.execute_reply":"2024-05-22T10:19:09.392934Z"},"papermill":{"duration":0.286758,"end_time":"2024-05-22T10:19:09.39583","exception":false,"start_time":"2024-05-22T10:19:09.109072","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vision_transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"vision_transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ patch_embedding                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbedding</span>)                │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_1             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_2             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_3             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_4             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_5             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ layer_normalization_12          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ patch_embedding                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mPatchEmbedding\u001b[0m)                │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_1             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_2             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_3             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_4             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ transformer_block_5             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ layer_normalization_12          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,688</span> (198.00 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,688\u001b[0m (198.00 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,688</span> (198.00 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,688\u001b[0m (198.00 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["# Define your model parameters (Reduced complexity)\n","image_size = (224, 224, 3)\n","patch_size = 16\n","embed_dim = 256\n","num_heads = 8\n","num_blocks = 6\n","mlp_dim = 256\n","num_classes = 1\n","dropout_rate = 0.1\n","learning_rate = 1e-4\n","\n","vit_model = VisionTransformer(image_size, patch_size, embed_dim, num_heads, num_blocks, mlp_dim, num_classes, dropout_rate)\n","vit_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","vit_model.summary()"]},{"cell_type":"code","execution_count":6,"id":"baa2ff01","metadata":{"execution":{"iopub.execute_input":"2024-05-22T10:19:09.40863Z","iopub.status.busy":"2024-05-22T10:19:09.408308Z","iopub.status.idle":"2024-05-22T10:21:45.986809Z","shell.execute_reply":"2024-05-22T10:21:45.985686Z"},"papermill":{"duration":156.587144,"end_time":"2024-05-22T10:21:45.988767","exception":false,"start_time":"2024-05-22T10:19:09.401623","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1716373180.137797      67 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","W0000 00:00:1716373180.268941      67 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m 51/289\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 339ms/step - accuracy: 0.5478 - loss: nan"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1716373197.259212      67 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 236ms/step - accuracy: 0.5318 - loss: nan - val_accuracy: 0.5905 - val_loss: nan\n","Epoch 2/2\n"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1716373248.282537      69 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 196ms/step - accuracy: 0.5015 - loss: nan - val_accuracy: 0.5905 - val_loss: nan\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7ea92c46ec80>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Train the model\n","epochs = 2\n","vit_model.fit(train_generator, epochs=epochs, validation_data=val_generator)"]},{"cell_type":"code","execution_count":null,"id":"48d80f7d","metadata":{"papermill":{"duration":0.053999,"end_time":"2024-05-22T10:21:46.099587","exception":false,"start_time":"2024-05-22T10:21:46.045588","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4854718,"sourceId":8201044,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":277.269581,"end_time":"2024-05-22T10:21:49.216643","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-22T10:17:11.947062","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}